{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME538 - Introduction to Data Science\n",
    "## Assignment 9 - Unsupervised Learning\n",
    "\n",
    "### Learning Objectives\n",
    "After completing this assignment, you should be comfortable:\n",
    "\n",
    "- Using Principal Component Analysis\n",
    "- Using k-means clustering\n",
    "\n",
    "### Marking Breakdown\n",
    "\n",
    "Question | Points\n",
    "--- | ---\n",
    "Question 1a | 1\n",
    "Question 1b | 1\n",
    "Question 1c | 1\n",
    "Question 2a | 1\n",
    "Question 2b | 1\n",
    "Question 2c | 1\n",
    "Question 2d | 1\n",
    "Question 2e | 1\n",
    "Question 2f | 1\n",
    "Question 2g | 1\n",
    "Question 2h | 1\n",
    "Question 2i | 1\n",
    "Question 2j | 3\n",
    "Total | 15\n",
    "\n",
    "One of the following marks below will be added to the **Total** above.\n",
    "\n",
    "### Code Quality\n",
    "\n",
    "| Rank | Points | Description |\n",
    "| :-- | :-- | :-- |\n",
    "| Youngling | 1 | Code is unorganized, variables names are not descriptive, redundant, memory-intensive, computationally-intensive, uncommented, error-prone, difficult to understand. |\n",
    "| Padawan | 2 | Code is organized, variables names are descriptive, satisfactory utilization of memory and computational resources, satisfactory commenting, readable. |\n",
    "| Jedi | 3 | Code is organized, easy to understand, efficient, clean, a pleasure to read. #cleancode |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import 3rd party libraries\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configure Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PCA on 3D Data\n",
    "Let's import `data_3d.csv`. We have named the DataFrame surfboard because the data resembles a surfboard when plotted in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surfboard = pd.read_csv('data_3d.csv')\n",
    "surfboard.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will allow you to view the data as a 3D scatterplot. Rotate the data around and zoom in and out using your trackpad or the controls at the top right of the figure.\n",
    "\n",
    "You should see that the data is an ellipsoid that looks roughly like a `surfboard` or a hashbrown patty. That is, it is pretty long in one direction, pretty wide in another direction, and relatively thin along its third dimension. We can think of these as the \"length\", \"width\", and \"thickness\" of the `surfboard` data.\n",
    "\n",
    "Observe that the `surfboard` is not aligned with the `x/y/z` axes.\n",
    "\n",
    "To give the figure a little more visual pop we assigned a pre-determined color value (that we've arbitrarily chosen) to each point. These colors do not mean anything important, they're simply there as a visual aid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_surfboard_data(df):\n",
    "    colors = pd.read_csv(\"surfboard_colors.csv\", header = None).values\n",
    "    df_copy = df.copy()\n",
    "    df_copy.insert(loc = 3, column = \"color\", value = colors)\n",
    "    return df_copy\n",
    "    \n",
    "fig = px.scatter_3d(colorize_surfboard_data(surfboard), x='x', y='y', z='z', range_x = [-10, 10], range_y = [-10, 10], range_z = [-10, 10], color = \"color\", color_continuous_scale = 'RdBu')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1a\n",
    "Now that we've understood the data, let's work on understanding what PCA will do when applied to this data.\n",
    "\n",
    "To properly perform PCA, we will first need to \"center\" the data so that the mean of each feature is 0.\n",
    "\n",
    "Compute the columnwise mean of `surfboard` in the cell below, and store the result in `surfboard_mean`. You can choose to make `surfboard_mean` a numpy array or a series, whichever is more convenient for you. Regardless of what data type you use, `surfboard_mean` should have 3 means, 1 for each attribute, with the `x` coordinate first, then `y`, then `z`.\n",
    "\n",
    "Then, subtract `surfboard_mean` from `surfboard`, and save the result in `surfboard_centered`. The order of the columns in `surfboard_centered` should be x, then y, then z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "surfboard_mean = ...\n",
    "surfboard_centered = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1b\n",
    "Using the Principal Component funtionality in `Scikit-Learn`, initialize a `PCA` object and assign it to a variable called `pca`. Next, fit the `PCA` model using the `surfboard_centered` DataFrame. You should be computing as many principal compnents as you have features in `surfboard_centered`, which is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "pca = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1c\n",
    "Lastly, you must use the Principal Components you just computed to transform `surfboard_centered` into the new Principal Components coordinate space. The transformed DataFrame should be assigned to the variable `surfboard_pcs`. The column names should be as follows: `pc1, pc2, and pc3`. Your DataFrame should look something like this.\n",
    "<br>\n",
    "<img src=\"images/q1c.png\" alt=\"drawing\" width=\"300\"/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "surfboard_pcs = ...\n",
    "\n",
    "# View DataFrame\n",
    "surfboard_pcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the original data to Principal Component space is simply a rotation of the data such that the data will now appear \"axis aligned\". Specifically, for a 3d dataset, if we plot `PC1`, `PC2`, and `PC3` along the `x`, `y`, and `z` axes of our plot, then the greatest amount of variation happens along the `x-axis`, the second greatest amount along the `y-axis`, and the smallest amount along the `z-axis`.\n",
    "\n",
    "To visualize this, run the cell below, which will show our data now projected onto the principal component space. Compare with your original figure, and observe that the data is exactly the same, only it is now rotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(colorize_surfboard_data(surfboard_pcs), \n",
    "                    x='pc1', y='pc2', z='pc3', range_x = [-10, 10], range_y = [-10, 10], range_z = [-10, 10], color = 'color', color_continuous_scale = 'RdBu');\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a 2D scatterplot of our surfboard data as well. Note that the resulting is just the 3D plot as viewed from directly \"overhead\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data = colorize_surfboard_data(surfboard_pcs), x = 'pc1', y = 'pc2', hue = \"color\", palette = \"RdBu\", legend = False)\n",
    "ax.set_xlim(-10, 10);\n",
    "ax.set_ylim(-10, 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fremont Bridge Bike Traffic\n",
    "The data we will use here are the hourly bicycle counts on Seattle's Fremont Bridge. These data come from an automated bicycle counter, installed in late 2012, which has inductive sensors under the sidewalks on either side of the bridge.\n",
    "\n",
    "First, let's import the ride count dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data = pd.read_csv('fremont_bridge_bicycle_counter.csv', \n",
    "                        index_col='datetime', parse_dates=True)\n",
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some quick data cleaning and set any missing values to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data = bike_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2a\n",
    "Next, let's plot the number of weekly rides for both direction for the entire duration of the dataset. You're plot should look something like this.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/q2a.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2b\n",
    "Describe what you see in this plot in terms of trend, seasonality, and cyclicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we could do a variety of other visualizations based on our intuition about what might affect bicycle counts. For example, we could look at the effect of the days of the week, the effect of the weather, the effect of COVID and other factors that we explored previously in Assignment 4. But we could also proceed by letting the dataset speak for itself, and use unsupervised machine learning techniques to learn what the data have to tell us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2c\n",
    "For this, we will consider each day in the dataset as its own separate entity (sample/row/record). For each day, we have 48 observations: two observations (east and west directions) for each of the 24 hours in a day. By examining the days in light of these observations and doing some careful analysis, we should be able to extract meaningful quantitative statements from the data themselves, without the need to lean on any other assumptions. Assumptions such as: The day of the week impacts ridership or if its hot out, well people will be riding their bikes.\n",
    "\n",
    "The first step in this approach is to transform our data. We want DataFrame, where each row corresponds to a day, and each column corresponds to one of the 48 observations. The DataFrame index should be a Datetime object. We can arrange the data this way using the `pivot_table()` function in `Pandas`. Your output should look something like this.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/q2c.png\" alt=\"drawing\" width=\"700\"/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "bike_data_features = ...\n",
    "\n",
    "# View DataFrame\n",
    "bike_data_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When displaying the shape of `bike_data_features` we should get `(3376, 48)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we'll do is compute the Principal Components of `bike_data_features` and transform the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2d\n",
    "Using the Principal Component funtionality in `Scikit-Learn`, initialize a `PCA` object and assign it to a variable called `pca`. Next, fit the `PCA` model using the `bike_data_features` DataFrame. You should be computing as many principal compnents as you have features in `bike_data_features`, which is 48."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "pca = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2e\n",
    "Lastly, you must use the Principal Components you just computed to transform `bike_data_features` into the new Principal Components coordinate space. The transformed DataFrame should be assigned to the variable `bike_data_features_pcs`. The index should be the same as for `bike_data_features` and the column names should be as follows: `pc1, pc2, pc3, ..., pc48`. Your DataFrame should look something like this.\n",
    "<br>\n",
    "<img src=\"images/q2d.png\" alt=\"drawing\" width=\"900\"/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "bike_data_features_pcs = ...\n",
    "\n",
    "# View DataFrame\n",
    "bike_data_features_pcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2f\n",
    "Using the fitted `pca` model, create a DataFrame called `pca_summary` which contained information about the explained variable of each Principal Component. There should be three rows (`Variance`, `Proportion of Variance`, `Cumulative Proportion`) and 48 columns (`PC1, PC2, ..., PC48`). `pca_summary` should look something like this.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/q2f.png\" alt=\"drawing\" width=\"900\"/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "pca_summary = ...\n",
    "\n",
    "# View DataFrame\n",
    "pca_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2g\n",
    "Next, create a plot showing `Proportion of Variance` and `Cumulative Proportion` for the first 10 Principal Components. You're plot should look something like this.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/q2g.png\" alt=\"drawing\" width=\"400\"/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2h\n",
    "From the previous plot, we can see that `PC1` and `PC2` describe at roughly 90% of the total variance in the dataset. While 48-dimensional data is difficult to plot, we certainly know how to plot two-dimensional data. Create a simple scatter plot, and for reference we'll color each point according to the total number of trips taken that day.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/q2h.png\" alt=\"drawing\" width=\"400\"/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the days lie in 3 or 4 quite distinct groups, and that the total number of trips increases along the length of each projected cluster. Further, the groups begin to be less distinguishable when the number of trips during the day is very small.\n",
    "\n",
    "This is very interesting. From the raw data, we can determine that there are basically a few primary types of days for Seattle bicyclists. Let's model these clusters and try to figure out what these types-of-day are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2i\n",
    "When you have groups of data you'd like to automatically separate, but no previously-determined labels for the groups, the type of algorithm you are looking at is a clustering algorithm. There are a number of clustering algorithms out there but let's use `KMeans` because we learn about it last week.\n",
    "\n",
    "Fit 10 `KMeans` models (num clusters: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10) and plot their inertia to create a figure like this.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/q2i.png\" alt=\"drawing\" width=\"400\"/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the elbow plot above, we select 4 clusters to visualize. First, let's fit a 4 cluster model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(bike_data_features_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's plot the cluster labels with `PC1` and `PC2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trips = bike_data_features.sum(axis=1)\n",
    "sns.scatterplot(x=bike_data_features_transformed['pc1'], \n",
    "                y=bike_data_features_transformed['pc2'], \n",
    "                hue=kmeans.labels_.astype(str), alpha=0.5)\n",
    "plt.xlabel('PC1', fontsize=14)\n",
    "plt.ylabel('PC2', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, we can clearly see that `KMeans` is not the most appropriate method. There are a number of clustering algorithms out there, but for nicely-defined oval-shaped blobs like we see above, Gaussian Mixture Models are a very good choice. An important lesson here is that you will likely need to experiment with different clustering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Using Gaussian Mixture Models\n",
    "Let's use `GaussianMixture` from `Scikit-Learn` with 4 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=4, covariance_type='tied', random_state=0)\n",
    "gmm.fit(bike_data_features_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's plot the cluster labels with `PC1` and `PC2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trips = bike_data_features.sum(axis=1)\n",
    "sns.scatterplot(x=bike_data_features_transformed['pc1'], \n",
    "                y=bike_data_features_transformed['pc2'], \n",
    "                hue=gmm.predict(bike_data_features_transformed).astype(str), alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join these inferred cluster labels to the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data_features['cluster'] = gmm.predict(bike_data_features_transformed)\n",
    "bike_data_new = bike_data.join(bike_data_features['cluster'], on=bike_data.index.date)\n",
    "bike_data_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can find the average trend by cluster and time using a `groupby` within this updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_hour = bike_data_new.groupby(['cluster', bike_data_new.index.time]).mean()\n",
    "by_hour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the average hourly trend among the days within each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "hourly_ticks = 4 * 60 * 60 * np.arange(6)\n",
    "\n",
    "for i in range(4):\n",
    "    by_hour.loc[i].plot(ax=ax[i], xticks=hourly_ticks)\n",
    "    ax[i].set_title('Cluster {0}'.format(i), fontsize=16)\n",
    "    ax[i].set_xlabel('Time', fontsize=14)\n",
    "    ax[i].set_ylabel('Average Hourly Trips', fontsize=14)\n",
    "    ax[i].tick_params(axis='x', labelrotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots give us some insight into the interpretation of the four clusters. Clusters 1 and 3 show a sharp bimodal traffic pattern with higher Eastward traffic in the morning and higher Westward traffic in the evening. For this reason, let's merge cluster 1 and 3 and replot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data_new[bike_data_new['cluster'] == 3] = 1\n",
    "bike_data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_hour = bike_data_new.groupby(['cluster', bike_data_new.index.time]).mean()\n",
    "by_hour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "hourly_ticks = 4 * 60 * 60 * np.arange(6)\n",
    "\n",
    "for i in range(3):\n",
    "    by_hour.loc[i].plot(ax=ax[i], xticks=hourly_ticks)\n",
    "    ax[i].set_title('Cluster {0}'.format(i), fontsize=16)\n",
    "    ax[i].set_xlabel('Time', fontsize=14)\n",
    "    ax[i].set_ylabel('Average Hourly Trips', fontsize=14)\n",
    "    ax[i].tick_params(axis='x', labelrotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2j\n",
    "From simple unsupervised dimensionality reduction and clustering, we've discovered three distinct classes of days in the data. Use whatever visualization, tables, etc. you want to explain these three classes of days. Use the datetime index to provide context.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Congratulation, you're done Assignment 9. Review your answers and clean up that code before submitting on Quercus. `#cleancode`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
